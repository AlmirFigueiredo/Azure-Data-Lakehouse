{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipeline Silver \u2192 Gold com Delta Lake\n",
        "\n",
        "Este notebook demonstra um pipeline de transforma\u00e7\u00e3o de dados da camada **Silver** para a camada **Gold** em um Data Lake, utilizando **Apache Spark** e **Delta Lake**.\n",
        "\n",
        "Os coment\u00e1rios em cada c\u00e9lula de c\u00f3digo descrevem, em terceira pessoa e de forma impessoal, o que \u00e9 realizado em cada etapa do processo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Nesta etapa s\u00e3o importadas as fun\u00e7\u00f5es necess\u00e1rias do PySpark.\n",
        "# S\u00e3o carregadas fun\u00e7\u00f5es de agrega\u00e7\u00e3o, manipula\u00e7\u00e3o de colunas e extra\u00e7\u00e3o de componentes de data,\n",
        "# que ser\u00e3o utilizadas na constru\u00e7\u00e3o da camada Gold.\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col, year, month, to_date,\n",
        "    count, sum, avg, max as fmax, min as fmin,\n",
        "    array_contains,\n",
        "    when\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Aqui \u00e9 inicializada a SparkSession configurada para trabalhar com Delta Lake.\n",
        "# Nessa configura\u00e7\u00e3o s\u00e3o definidos par\u00e2metros relacionados ao cat\u00e1logo Delta,\n",
        "# ao n\u00famero de parti\u00e7\u00f5es de shuffle e \u00e0 adapta\u00e7\u00e3o din\u00e2mica de planos de execu\u00e7\u00e3o.\n",
        "# A partir deste ponto, o ambiente distribu\u00eddo fica pronto para ler, transformar\n",
        "# e escrever dados nas camadas Silver e Gold.\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "         .appName(\"SilverToGold-Delta\")\n",
        "         .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
        "         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "         .config(\"spark.sql.shuffle.partitions\", \"128\")\n",
        "         .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "         .getOrCreate())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Agora \u00e9 definida a fun\u00e7\u00e3o 'get_paths', respons\u00e1vel por centralizar os caminhos\n",
        "# de leitura da camada Silver e de escrita da camada Gold no Data Lake.\n",
        "# Dessa forma, os endere\u00e7os ABFSS permanecem em um \u00fanico ponto de configura\u00e7\u00e3o,\n",
        "# facilitando ajustes entre ambientes.\n",
        "\n",
        "def get_paths():\n",
        "    storage = \"tccprojectdlstorage\"\n",
        "    # Caminho da tabela Silver em formato Delta.\n",
        "    silver = f\"abfss://silver@{storage}.dfs.core.windows.net/iot_events_delta\"\n",
        "    # Caminho da tabela Gold em formato Delta, onde ser\u00e3o armazenados os agregados di\u00e1rios.\n",
        "    gold = f\"abfss://gold@{storage}.dfs.core.windows.net/iot_daily_summary_delta\"\n",
        "    return silver, gold"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Em seguida \u00e9 criada a fun\u00e7\u00e3o 'load_silver', respons\u00e1vel pela leitura dos dados\n",
        "# da camada Silver diretamente no formato Delta. Ap\u00f3s a leitura, s\u00e3o selecionadas\n",
        "# apenas as colunas necess\u00e1rias para a camada Gold e aplicada uma l\u00f3gica de fallback\n",
        "# para garantir a exist\u00eancia das colunas derivadas de data.\n",
        "\n",
        "def load_silver(path):\n",
        "    print(f\"Lendo dados Silver (Delta Lake) de: {path}\")\n",
        "    df = spark.read.format(\"delta\").load(path)\n",
        "    \n",
        "    # Lista de colunas relevantes para o resumo di\u00e1rio na camada Gold.\n",
        "    cols_needed = [\n",
        "        \"event_ts\",\n",
        "        \"event_date\",\n",
        "        \"event_year\",\n",
        "        \"event_month\",\n",
        "        \"device_id\",\n",
        "        \"device_type\",\n",
        "        \"location_id\",\n",
        "        \"meta_location_group\",\n",
        "        \"meta_device_group\",\n",
        "        \"status_code\",\n",
        "        \"tags_array\",\n",
        "        \"temperature\",\n",
        "        \"pressure\",\n",
        "        \"energy_consumption\",\n",
        "        \"battery_level\",\n",
        "        \"dq_temp_out_of_range\",\n",
        "        \"dq_pressure_out_of_range\",\n",
        "        \"dq_battery_out_of_range\",\n",
        "    ]\n",
        "    \n",
        "    # Aqui s\u00e3o filtradas apenas as colunas que existem de fato no DataFrame,\n",
        "    # evitando falhas em cen\u00e1rios de evolu\u00e7\u00e3o de schema da camada Silver.\n",
        "    existing = [c for c in cols_needed if c in df.columns]\n",
        "    df = df.select(*existing)\n",
        "    \n",
        "    # Caso alguma coluna de data n\u00e3o esteja presente, ela \u00e9 reconstru\u00edda a partir do timestamp.\n",
        "    if \"event_date\" not in df.columns:\n",
        "        df = df.withColumn(\"event_date\", to_date(col(\"event_ts\")))\n",
        "    if \"event_year\" not in df.columns:\n",
        "        df = df.withColumn(\"event_year\", year(col(\"event_date\")))\n",
        "    if \"event_month\" not in df.columns:\n",
        "        df = df.withColumn(\"event_month\", month(col(\"event_date\")))\n",
        "    return df"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Nesta etapa \u00e9 definida a fun\u00e7\u00e3o 'transform_to_gold', que aplica as regras de neg\u00f3cio\n",
        "# e consolida os eventos em um n\u00edvel mais agregado para compor a camada Gold.\n",
        "#\n",
        "# Dentro dessa fun\u00e7\u00e3o s\u00e3o realizadas as seguintes a\u00e7\u00f5es principais:\n",
        "# - Defini\u00e7\u00e3o de uma m\u00e9trica bin\u00e1ria de criticidade por evento (is_critical);\n",
        "# - Convers\u00e3o das flags de qualidade de dados em inteiros (0/1) para permitir somat\u00f3rios;\n",
        "# - Agrupamento por data, dispositivo e localidade, com c\u00e1lculo de estat\u00edsticas como\n",
        "#   contagem de eventos, temperatura m\u00e9dia/m\u00e1xima/m\u00ednima, press\u00e3o m\u00e9dia, consumo m\u00e9dio\n",
        "#   de energia e n\u00edvel m\u00e9dio de bateria;\n",
        "# - C\u00e1lculo das contagens de viola\u00e7\u00f5es de qualidade e da porcentagem de eventos cr\u00edticos.\n",
        "\n",
        "def transform_to_gold(df):\n",
        "    print(\"Iniciando transforma\u00e7\u00f5es (Gold)...\")\n",
        "    \n",
        "    # Primeiro \u00e9 verificado se a coluna 'tags_array' est\u00e1 dispon\u00edvel para ser usada na\n",
        "    # defini\u00e7\u00e3o de criticidade dos eventos.\n",
        "    has_tags_array = \"tags_array\" in df.columns\n",
        "    crit_expr = col(\"status_code\") != \"OK\"\n",
        "    if has_tags_array:\n",
        "        crit_expr = crit_expr | array_contains(col(\"tags_array\"), \"critical\")\n",
        "    \n",
        "    # Aqui \u00e9 criada a coluna bin\u00e1ria que indica se o evento \u00e9 cr\u00edtico.\n",
        "    df = df.withColumn(\"is_critical\", when(crit_expr, 1).otherwise(0))\n",
        "    \n",
        "    # Em seguida, as colunas de qualidade de dados s\u00e3o convertidas para inteiros,\n",
        "    # permitindo o uso direto em agrega\u00e7\u00f5es por soma.\n",
        "    df = df.withColumn(\"dq_temp_flag\", col(\"dq_temp_out_of_range\").cast(\"int\"))\n",
        "    df = df.withColumn(\"dq_pressure_flag\", col(\"dq_pressure_out_of_range\").cast(\"int\"))\n",
        "    df = df.withColumn(\"dq_battery_flag\", col(\"dq_battery_out_of_range\").cast(\"int\"))\n",
        "    \n",
        "    # Aqui \u00e9 realizada a etapa de agrega\u00e7\u00e3o em n\u00edvel de data, dispositivo e localidade.\n",
        "    grouped = (\n",
        "        df.groupBy(\n",
        "            \"event_date\",\n",
        "            \"event_year\",\n",
        "            \"event_month\",\n",
        "            \"device_id\",\n",
        "            \"device_type\",\n",
        "            \"location_id\",\n",
        "            \"meta_location_group\",\n",
        "            \"meta_device_group\"\n",
        "        )\n",
        "        .agg(\n",
        "            count(\"*\").alias(\"events_total\"),\n",
        "            sum(\"is_critical\").alias(\"events_critical\"),\n",
        "            avg(\"temperature\").alias(\"avg_temperature\"),\n",
        "            fmax(\"temperature\").alias(\"max_temperature\"),\n",
        "            fmin(\"temperature\").alias(\"min_temperature\"),\n",
        "            avg(\"pressure\").alias(\"avg_pressure\"),\n",
        "            avg(\"energy_consumption\").alias(\"avg_energy_consumption\"),\n",
        "            avg(\"battery_level\").alias(\"avg_battery_level\"),\n",
        "            sum(\"dq_temp_flag\").alias(\"dq_temp_out_of_range_count\"),\n",
        "            sum(\"dq_pressure_flag\").alias(\"dq_pressure_out_of_range_count\"),\n",
        "            sum(\"dq_battery_flag\").alias(\"dq_battery_out_of_range_count\")\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Por fim, \u00e9 calculada a porcentagem de eventos cr\u00edticos dentro de cada grupo agregado.\n",
        "    grouped = grouped.withColumn(\n",
        "        \"pct_critical_events\",\n",
        "        (col(\"events_critical\") / col(\"events_total\")).cast(\"double\")\n",
        "    )\n",
        "    print(\"Transforma\u00e7\u00f5es Gold conclu\u00eddas.\")\n",
        "    return grouped"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Agora \u00e9 definida a fun\u00e7\u00e3o 'write_gold', respons\u00e1vel por gravar o resultado agregado\n",
        "# na camada Gold, utilizando o formato Delta. A escrita \u00e9 realizada em modo 'overwrite'\n",
        "# e particionada por ano e m\u00eas, o que favorece consultas anal\u00edticas filtradas por tempo.\n",
        "# A op\u00e7\u00e3o 'overwriteSchema' permite que altera\u00e7\u00f5es estruturais sejam refletidas na tabela.\n",
        "\n",
        "def write_gold(df, path):\n",
        "    print(f\"Iniciando escrita para a camada Gold (Delta Lake) em: {path}\")\n",
        "    (\n",
        "        df.write\n",
        "          .mode(\"overwrite\")\n",
        "          .partitionBy(\"event_year\", \"event_month\")\n",
        "          .format(\"delta\")\n",
        "          .option(\"overwriteSchema\", \"true\")\n",
        "          .save(path)\n",
        "    )\n",
        "    print(\"Escrita na camada Gold conclu\u00edda.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Nesta c\u00e9lula \u00e9 executado o pipeline Silver \u2192 Gold de ponta a ponta.\n",
        "# S\u00e3o realizadas as seguintes etapas:\n",
        "# 1. Recupera\u00e7\u00e3o dos caminhos das tabelas Silver e Gold;\n",
        "# 2. Leitura da camada Silver em formato Delta;\n",
        "# 3. Aplica\u00e7\u00e3o das transforma\u00e7\u00f5es e agrega\u00e7\u00f5es para gerar o resumo di\u00e1rio;\n",
        "# 4. Escrita do resultado em uma tabela Delta particionada por ano e m\u00eas.\n",
        "\n",
        "silver_path, gold_path = get_paths()\n",
        "\n",
        "df_silver = load_silver(silver_path)\n",
        "df_gold = transform_to_gold(df_silver)\n",
        "write_gold(df_gold, gold_path)\n",
        "\n",
        "print(\"Pipeline S->G (Delta) finalizado com sucesso.\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}