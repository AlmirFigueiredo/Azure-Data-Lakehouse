{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline Bronze → Silver com Delta Lake\n",
        "\n",
        "Este notebook demonstra, em alto nível, um pipeline de transformação de dados da camada **Bronze** para a camada **Silver** em um Data Lake, utilizando **Apache Spark** e **Delta Lake**.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa as classes e funções necessárias do PySpark que serão utilizados no pipeline.\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import (\n",
        "    StructType, StructField,\n",
        "    LongType, StringType, TimestampType, DoubleType\n",
        ")\n",
        "from pyspark.sql.functions import (\n",
        "    col, to_date, hour,\n",
        "    split, array_distinct,\n",
        "    regexp_extract,\n",
        "    year, month\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializa a SparkSession já configurada para trabalhar com Delta Lake.\n",
        "# Nesta etapa, o ambiente de execução distribuída é criado, permitindo que\n",
        "# o pipeline leia, transforme e escreva dados de forma paralela.\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "         .appName(\"BronzeToSilver-Delta-Full\")\n",
        "         .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
        "         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "         .config(\"spark.sql.shuffle.partitions\", \"128\")\n",
        "         .getOrCreate())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a função de utilidade 'get_paths', responsável por centralizar os caminhos\n",
        "# de leitura (camada Bronze) e escrita (camada Silver) no Data Lake.\n",
        "def get_paths():\n",
        "    storage = \"tccprojectdlstorage\"\n",
        "    \n",
        "    # O '*.parquet' indica que todos os arquivos Parquet na camada Bronze\n",
        "    # serão lidos em um único DataFrame, independentemente de subpastas lógicas.\n",
        "    bronze = f\"abfss://bronze@{storage}.dfs.core.windows.net/*.parquet\"\n",
        "    \n",
        "    # O destino é uma tabela única na camada Silver, representada como uma tabela Delta.\n",
        "    silver = f\"abfss://silver@{storage}.dfs.core.windows.net/iot_events_delta\"\n",
        "    return bronze, silver"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o schema esperado para os dados da camada Bronze, garantindo consistência tipada\n",
        "# entre execuções, evita inferência automática (que pode ser custosa) e facilita\n",
        "# o controle de evolução de campos ao longo do tempo.\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"event_id\", LongType()),\n",
        "    StructField(\"device_id\", StringType()),\n",
        "    StructField(\"device_type\", StringType()),\n",
        "    StructField(\"location_id\", StringType()),\n",
        "    StructField(\"event_ts\", TimestampType()),\n",
        "    StructField(\"temperature\", DoubleType()),\n",
        "    StructField(\"pressure\", DoubleType()),\n",
        "    StructField(\"energy_consumption\", DoubleType()),\n",
        "    StructField(\"battery_level\", DoubleType()),\n",
        "    StructField(\"status_code\", StringType()),\n",
        "    StructField(\"tags\", StringType()),\n",
        "    StructField(\"payload\", StringType())\n",
        "])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementa a função 'load_bronze', responsável por ler todos os dados Parquet\n",
        "# da camada Bronze usando o schema definido. A opção 'mergeSchema' permite que o\n",
        "# Spark una pequenas variações de schema entre arquivos, o que é útil em cenários\n",
        "# onde o modelo de dados evolui com o tempo.\n",
        "\n",
        "def load_bronze(path):\n",
        "    print(f\"Lendo TODOS os dados Bronze de: {path}\")\n",
        "    return (spark.read\n",
        "            .schema(schema)\n",
        "            .option(\"mergeSchema\", \"true\")\n",
        "            .parquet(path))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a função de transformação principal, responsável por converter os dados\n",
        "# da camada Bronze em um modelo Silver mais analítico e limpo.\n",
        "#\n",
        "# Nesta etapa:\n",
        "# - Cria colunas derivadas de tempo (data, ano, mês, hora);\n",
        "# - Aplica regras de qualidade de dados (Data Quality) em temperatura, pressão e bateria;\n",
        "# - Normaliza o campo de tags em um array de strings únicas;\n",
        "# - Extrai metadados estruturados a partir do campo 'payload' usando expressões regulares.\n",
        "\n",
        "def transform(df):\n",
        "    print(\"Iniciando transformações (Silver)...\")\n",
        "    \n",
        "    # Cria colunas derivadas de tempo para facilitar filtros, agregações e particionamento.\n",
        "    df = df.withColumn(\"event_date\", to_date(col(\"event_ts\")))\n",
        "    df = df.withColumn(\"event_year\", year(col(\"event_ts\")))\n",
        "    df = df.withColumn(\"event_month\", month(col(\"event_ts\")))\n",
        "    df = df.withColumn(\"event_hour\", hour(col(\"event_ts\")))\n",
        "    \n",
        "    # Aplica regras de qualidade para identificar leituras inválidas ou suspeitas.\n",
        "    df = df.withColumn(\"dq_temp_out_of_range\", (col(\"temperature\") < -50) | (col(\"temperature\") > 100))\n",
        "    df = df.withColumn(\"dq_pressure_out_of_range\", col(\"pressure\") <= 0)\n",
        "    df = df.withColumn(\"dq_battery_out_of_range\", (col(\"battery_level\") < 0) | (col(\"battery_level\") > 100))\n",
        "    \n",
        "    # Converte a string de tags em um array de tags únicas, removendo duplicatas.\n",
        "    df = df.withColumn(\"tags_array\", array_distinct(split(col(\"tags\"), \",\")))\n",
        "    \n",
        "    # Extrai metadados estruturados a partir do campo 'payload' com expressões regulares.\n",
        "    df = df.withColumn(\"fw_version\", regexp_extract(col(\"payload\"), r\"fw=([^;]+);\", 1))\n",
        "    df = df.withColumn(\"meta_location_group\", regexp_extract(col(\"payload\"), r\"location_group=([^;]+);\", 1))\n",
        "    df = df.withColumn(\"meta_device_group\", regexp_extract(col(\"payload\"), r\"device_group=([^;]+);\", 1))\n",
        "    \n",
        "    print(\"Transformações concluídas.\")\n",
        "    return df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementa a função 'write_silver', que materializa os dados transformados\n",
        "# em uma tabela Delta na camada Silver. Nesta escrita:\n",
        "# - Sobrescrever o conjunto de dados existente (modo 'overwrite');\n",
        "# - Particionar fisicamente por ano e mês do evento (event_year, event_month),\n",
        "#   melhorando a eficiência de consultas filtradas por tempo;\n",
        "# - Utilizar o formato Delta, que adiciona controle transacional e suporte a ACID;\n",
        "# - Permitir a evolução de schema com a opção 'overwriteSchema'.\n",
        "\n",
        "def write_silver(df, path):\n",
        "    print(f\"Iniciando escrita para a camada Silver (Delta Lake) em: {path}\")\n",
        "    (df.write\n",
        "     .mode(\"overwrite\")\n",
        "     .partitionBy(\"event_year\", \"event_month\")\n",
        "     .format(\"delta\")\n",
        "     .option(\"overwriteSchema\", \"true\")\n",
        "     .save(path))\n",
        "    print(\"Escrita na camada Silver concluída.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Executa o pipeline fim-a-fim, encadeando as funções de leitura, transformação\n",
        "# e escrita. Esta célula representa o fluxo operacional:\n",
        "# 1. Descobrir caminhos das camadas Bronze e Silver;\n",
        "# 2. Ler todos os dados da Bronze em um DataFrame;\n",
        "# 3. Aplicar transformações e regras de qualidade para gerar a Silver;\n",
        "# 4. Persistir o resultado em uma tabela Delta particionada por ano e mês.\n",
        "\n",
        "bronze_path, silver_path = get_paths()\n",
        "\n",
        "df_bronze = load_bronze(bronze_path)\n",
        "df_silver = transform(df_bronze)\n",
        "write_silver(df_silver, silver_path)\n",
        "\n",
        "print(\"Pipeline B->S (Delta) finalizado com sucesso.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}