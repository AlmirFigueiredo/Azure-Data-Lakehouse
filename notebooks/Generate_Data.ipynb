{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerador de Dados de Teste PySpark (IoT)\n",
    "Este notebook gera volumes de dados (**Small, Medium, Large**) otimizados para clusters com pouca memória RAM (ex: nós de 8GB), utilizando a técnica de escrita em chunks para evitar erros de OOM (Out of Memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, rand, randn, expr, \n",
    "    current_timestamp,\n",
    "    array, when,\n",
    "    monotonically_increasing_id\n",
    ")\n",
    "\n",
    "# 1. Inicialização do Spark\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"DataGenerator-TCC-Parquet\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"128\")\n",
    "         .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configurações de Storage\n",
    "storage_account = \"tccprojectdlstorage\"\n",
    "base_path = f\"abfss://source@{storage_account}.dfs.core.windows.net\"\n",
    "temp_path = f\"{base_path}/_temp_calibration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Definição do Schema e Gerador\n",
    "def generate_data(num_rows):\n",
    "    \"\"\"Gera um DataFrame com o schema do pipeline de IoT.\"\"\"\n",
    "    \n",
    "    df = spark.range(num_rows).withColumnRenamed(\"id\", \"event_id\")\n",
    "    \n",
    "    df = df.withColumn(\"device_id\", expr(\"uuid()\"))\n",
    "    \n",
    "    df = df.withColumn(\"device_type\", \n",
    "                       array(lit('type-A'), lit('type-B'), lit('type-C'))[(rand() * 3).cast('int')])\n",
    "    \n",
    "    df = df.withColumn(\"location_id\", \n",
    "                       (rand() * 100).cast('int').cast('string'))\n",
    "    \n",
    "    df = df.withColumn(\"event_ts\", \n",
    "                       (current_timestamp().cast('long') - (rand() * 100000000)).cast('timestamp'))\n",
    "    \n",
    "    df = df.withColumn(\"temperature\", (randn() * 15) + 25)\n",
    "    df = df.withColumn(\"pressure\", (randn() * 50) + 101325)\n",
    "    df = df.withColumn(\"energy_consumption\", rand() * 10)\n",
    "    df = df.withColumn(\"battery_level\", rand() * 100)\n",
    "    \n",
    "    df = df.withColumn(\"status_code\", \n",
    "                       when(rand() < 0.9, \"OK\").otherwise(\"ERROR\"))\n",
    "    \n",
    "    df = df.withColumn(\"tags\", \n",
    "                       when(rand() < 0.1, \"critical,high\").otherwise(\"normal,low\"))\n",
    "    \n",
    "    df = df.withColumn(\"payload\", \n",
    "                       when(rand() < 0.5, \"fw=1.2.3;location_group=A;device_group=1\")\n",
    "                       .otherwise(\"fw=1.2.4;location_group=B;device_group=2\"))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Funções Utilitárias de Sistema de Arquivos\n",
    "def get_fs_path_size(path_str):\n",
    "    try:\n",
    "        fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "            spark._jvm.java.net.URI.create(path_str),\n",
    "            spark._jsc.hadoopConfiguration()\n",
    "        )\n",
    "        path_jvm = spark._jvm.org.apache.hadoop.fs.Path(path_str)\n",
    "        size_bytes = fs.getContentSummary(path_jvm).getLength()\n",
    "        return size_bytes\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao obter tamanho do {path_str}: {e}\")\n",
    "        return 0\n",
    "\n",
    "def cleanup(path_str):\n",
    "    print(f\"Limpando diretório: {path_str}\")\n",
    "    try:\n",
    "        fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "            spark._jvm.java.net.URI.create(path_str),\n",
    "            spark._jsc.hadoopConfiguration()\n",
    "        )\n",
    "        path_jvm = spark._jvm.org.apache.hadoop.fs.Path(path_str)\n",
    "        if fs.exists(path_jvm):\n",
    "            fs.delete(path_jvm, True)\n",
    "    except Exception as e:\n",
    "        print(f\"Falha ao limpar {path_str}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Calibração\n",
    "print(\"Iniciando calibração para estimar o tamanho dos dados...\")\n",
    "CALIBRATION_ROWS = 1_000_000\n",
    "GB = 1024 * 1024 * 1024\n",
    "MB = 1024 * 1024\n",
    "\n",
    "TARGET_FILE_SIZE_MB = 128 \n",
    "TARGET_CHUNK_GB = 1       \n",
    "\n",
    "df_sample = generate_data(CALIBRATION_ROWS)\n",
    "df_sample.repartition(1).write.mode(\"overwrite\").parquet(temp_path)\n",
    "\n",
    "size_bytes = get_fs_path_size(temp_path)\n",
    "size_per_row = size_bytes / CALIBRATION_ROWS\n",
    "\n",
    "rows_per_chunk = int((TARGET_CHUNK_GB * GB) / size_per_row)\n",
    "partitions_per_chunk = int((TARGET_CHUNK_GB * GB) / (TARGET_FILE_SIZE_MB * MB))\n",
    "partitions_per_chunk = max(1, partitions_per_chunk)\n",
    "\n",
    "print(f\"Calibração concluída: {size_per_row:.8f} bytes/linha.\")\n",
    "print(f\"Cada chunk de 1GB terá ~{rows_per_chunk:,} linhas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Execução da Geração\n",
    "scales = [\n",
    "    (\"small\", 10), \n",
    "    (\"medium\", 40), \n",
    "    (\"large\", 100)\n",
    "]\n",
    "\n",
    "for scale_name, total_gb in scales:\n",
    "    output_path = f\"{base_path}/{scale_name}\"\n",
    "    cleanup(output_path)\n",
    "    \n",
    "    print(f\"\\n--- Iniciando Geração para '{scale_name}' ({total_gb} GB) ---\")\n",
    "    \n",
    "    for i in range(total_gb):\n",
    "        print(f\"Gerando chunk {i+1} de {total_gb}...\")\n",
    "        df_chunk = generate_data(rows_per_chunk)\n",
    "        df_chunk = df_chunk.repartition(partitions_per_chunk)\n",
    "        df_chunk.write.mode(\"append\").parquet(output_path)\n",
    "\n",
    "cleanup(temp_path)\n",
    "print(\"--- GERAÇÃO DE DADOS CONCLUÍDA ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Python 3)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}